{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notebook anterior, nós aprendemos intuitivamente como o perceptron aprende. De maneira geral, nós vamos atualizando os pesos e o bias sempre buscando diminuir uma função de custo. Nesse notebook, nós vamos ver como esse aprendizado realmente acontence, tanto na teoria quanto na prática. Também utilizaremos o Perceptron para resolver problemas de classificação e regressão.\n",
    "\n",
    "__Objetivos__:\n",
    "\n",
    "- Implementar o perceptron e seu modelo de aprendizado em python puro, Numpy, Keras e Tensorflow\n",
    "- Utilizar o perceptron para regressão e classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Introdução](#Introdução)\n",
    "- [Regra de Aprendizado do Perceptron](#Regra-de-Aprendizado-do-Perceptron)\n",
    "- [Pseudo-algoritmo do Perceptron](#Pseudo-algoritmo-do-Perceptron)\n",
    "\n",
    "[Classificação](#Classificação)\n",
    "- [Porta AND/OR](#Porta-AND/OR)\n",
    "- [Exercício de Classificação](#Exerc%C3%ADcio-de-Classificação)\n",
    "\n",
    "[Regressão](#Regressão)\n",
    "- [Exercício de Regressão](#Exerc%C3%ADcio-de-Regressão)\n",
    "\n",
    "[Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T12:53:30.345746Z",
     "start_time": "2017-09-20T12:52:48.057739Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from random import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "O tipo mais básico de Rede Neural Artificial é formada por apenas um neurônio, o __Perceptron__. Inicialmente, o Perceptron foi projetado para ser um __classificador binário linear__ responsável por mapear uma ou mais entradas em uma saída desejada. Porém, também podemos utilizá-lo para resolver problemas de __regressão linear__. Ele foi projetado em 1957 por Frank Rosenblatt.\n",
    "\n",
    "O perceptron é formado por:\n",
    "\n",
    "<img src='images/perceptron.png' width='350'>\n",
    "\n",
    "- __entradas__ $x_1 ... x_D$: representam os atributos dos seus dados com dimensionalidade $D$. O Perceptron aceita qualquer tamanho de entrada, porém a saída é sempre apenas um valor.\n",
    "- __junção aditiva__ $\\sum$: também chamada de _função agregadora_, nada mais é que a soma ponderada das entradas com os __pesos__ ($w_1 ... w_D)$. Em geral, o resultado é somado com um __bias__ $b$, responsável por deslocar o resultado do somatório. A junção aditiva é descrita pela seguinte fórmula:\n",
    "\n",
    "$$\\sum_i^D{x_iw_i} + b$$\n",
    "\n",
    "- __função de ativação__ $f$: utilizada para mapear o resultado da junção aditiva em uma saída esperada. Mais detalhes abaixo.\n",
    "\n",
    "Logo, o Perceptron é representado pela seguinte fórmula matemática:\n",
    "\n",
    "$$y_{pred_i} = f(\\sum_i^D{x_iw_i} + b)$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $D$: representa a dimensionalidade das amostras, ou seja, a quantidade de atributos de cada amostra.\n",
    "- $x_i$: representam os atributos dos nossos dados que servem de entrada para o Perceptron.\n",
    "- $w_i$: representam os __pesos sinápticos__ que ponderam as entradas.\n",
    "- $b$: representa o __bias__, responsável por deslocar a fronteira de decisão além da origem e não depende de nenhum valor de entrada. Repare que o bias encontra-se fora do somatório.\n",
    "- $f$: __função de ativação__. Quando a função de ativação é linear, ou seja, nenhuma transformação é aplicada no resultado da junção aditiva, o Perceptron atua como um __Regressor Linear__. Se precisamos efetuar uma __Classificação binária__, devemos utilizar a função _step_ (também conhecida como _função degrau_) para mapear a saída em um valor discreto (0 ou 1):\n",
    "\n",
    "$$y_{pred} = \\begin{cases}1 & se \\ wx+b > 0\\\\0 & caso \\ contr\\acute ario\\end{cases}$$\n",
    "\n",
    "- $y_{pred}$: representa a saída do Perceptron (o valor predito).\n",
    "\n",
    "__Observações importantes__:\n",
    "\n",
    "- O Perceptron não faz __Classificação Multiclasse__.\n",
    "- __A atualização dos pesos é *online*, ou seja, efetuada amostra a amostra__ utilizando uma fórmula pré-definida que veremos na seção a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regra de Aprendizado do Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Perceptron tem sua própria forma de aprendizado conforme definido no seu artigo original. Na verdade, a fórmula para atualização dos pesos e bias é bem simples:\n",
    "\n",
    "$$w_i = w_i + \\lambda(y_i - y_{pred_i})x_i$$\n",
    "$$b_i = b_i + \\lambda(y_i - y_{pred_i})\\ \\ $$\n",
    "\n",
    "Onde $\\lambda$ é a __taxa de aprendizagem__.\n",
    "\n",
    "Repare que $y_i - y_{pred_i}$ significa calcular a diferença entre o valor esperado ($y_i$) e o valor predito ($y_{pred_i}$). Supondo que estamos fazendo __classificação binária__ de uma amostra $(x_i, y_i)$. Nesse caso, teremos duas possibilidades:\n",
    "- __O valor esperado é $y_i = y_{pred_i}$__, ou seja, a saída do Perceptron (após a função de ativação _step_) é __igual__ a saída esperada. Nesse caso, __a diferença $y_i - y_{pred_i} = 0$ e não haverá atualização de pesos__.\n",
    "- __O valor esperado é $y_i \\neq y_{pred_i}$__, ou seja, a saída do Perceptron (após a função de ativação _step_) é __diferente__ da saída esperada. Nesse caso, __a atualização dos pesos será dada pela diferença $y_i - y_{pred_i}$__. Repare que:\n",
    "    - quando essa diferença é __negativa__ (ou seja, $y_i = 0$ e $y_{pred_i} = 1$), __os pesos tendem a diminuir__.\n",
    "    - quando essa diferença é __positiva__ (ou seja, $y_i = 1$ e $y_{pred_i} = 0$), __os pesos tendem a aumentar__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-algoritmo do Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicialize os pesos $w_i$ e o bias _$b$\n",
    "2. Para cada amostra $(x_i, y_i)$ do nosso banco:\n",
    "    1. Calcule $y_{pred_i} = f(\\sum_i^D{x_iw_i} + b)$, onde $f$ é a __função _step_ para classificação__ e __linear no caso da regressão__\n",
    "    2. Calcule o $erro = y_i - y_{pred_i}$\n",
    "    3. Atualize os pesos $w_i = w_i + \\lambda*erro*x_i$\n",
    "    4. Atualize o bias $b_i = b_i + \\lambda*erro$\n",
    "3. Repita o passo 2 por N vezes ou até que $erro$ seja menor que um valor pré-determinado pelo usuário\n",
    "    \n",
    "Repare, como dito lá em cima, que __a atualização dos pesos e bias é feito a cada amostra__, e não somente após ver todas as amostras do banco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porta AND/OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:11:37.370366Z",
     "start_time": "2017-09-15T11:11:37.359356Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "#y = np.array([0, 1, 1, 1]) # porta OR\n",
    "y = np.array([[0, 0, 0, 1]]).T # porta AND\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:21:18.798586Z",
     "start_time": "2017-09-15T11:21:18.667487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)] # [1xD]\n",
    "b = 2*random() - 1 # [1x1]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(101):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        # insira seu código aqui\n",
    "        \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "print('y_pred: {0}'.format(np.dot(x, np.array(w))+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T12:21:02.603975Z",
     "start_time": "2017-09-15T12:21:02.555936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(100):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        # insira seu código aqui\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "print('y_pred: {0}'.format(np.dot(w, x.T)+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercício de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=1234)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_classifier(x, y, w, b):\n",
    "    x1_min, x1_max = x[:,0].min(), x[:,0].max()\n",
    "    x2_min, x2_max = x[:,1].min(), x[:,1].max()\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1_min-1, x1_max+1,100), np.linspace(x2_min-1, x2_max+1, 100))\n",
    "    x_mesh = np.array([x1.ravel(), x2.ravel()]).T\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], c=y, cmap='bwr')\n",
    "\n",
    "    y_mesh = np.dot(x_mesh, w.T) + b\n",
    "    y_mesh = np.where(y_mesh <= 0, 0, 1)\n",
    "\n",
    "    plt.contourf(x1, x2, y_mesh.reshape(x1.shape), cmap='bwr', alpha=0.5)\n",
    "    plt.xlim(x1_min-1, x1_max+1)\n",
    "    plt.ylim(x2_min-1, x2_max+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)] # [1xD]\n",
    "b = 2*random() - 1 # [1x1]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(1001):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        y_pred = sum([w[d]*x_i[d] for d in range(D)]) + b\n",
    "        y_pred = 1 if y_pred > 0 else 0\n",
    "        error = y_i[0] - y_pred\n",
    "        w = [w[d] + learning_rate*error*x_i[d] for d in range(D)]\n",
    "        b = b + learning_rate*error\n",
    "        sum_error += error\n",
    "        \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "plot_linear_classifier(x, y, np.array(w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(51):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        x_i = x_i.reshape(1, D)\n",
    "        y_pred = np.dot(x_i, w.T) + b \n",
    "        y_pred = np.where(y_pred > 0, 1, 0)\n",
    "        error = y_i - y_pred\n",
    "        w = w + learning_rate*np.dot(error.T, x_i)\n",
    "        b = b + learning_rate*error\n",
    "        sum_error += error\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "plot_linear_classifier(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regressão "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para transformar o Perceptron em um __regressor linear__, só o que temos de fazer é __remover a função de ativação _step___, transformando-a em uma função de ativação linear.\n",
    "\n",
    "Apesar dessa modificação, __a fórmula de atualização dos pesos não sofre nenhuma alteração__. \n",
    "\n",
    "Vamos, então, implementar nosso perceptron para classificação em Python, Numpy, Keras e TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:04.802972Z",
     "start_time": "2017-09-14T19:21:04.773952Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/medidas.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:08.765341Z",
     "start_time": "2017-09-14T19:21:08.441110Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = df.Altura.values\n",
    "y = df.Peso.values\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Altura')\n",
    "plt.ylabel('Peso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:10.893855Z",
     "start_time": "2017-09-14T19:21:10.883847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:11.535313Z",
     "start_time": "2017-09-14T19:21:11.527304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Exercício__: tentar estimar as learning_rates de __w__ e __b__. Elas são diferentes por que nossos dados não estão na mesma escala!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:38.253347Z",
     "start_time": "2017-09-14T19:21:16.413722Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*random() - 1 # [1xD]\n",
    "b = 2*random() - 1 # [1x1]\n",
    "\n",
    "for step in range(10001):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        # insira seu código aqui\n",
    "\n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:21:45.406815Z",
     "start_time": "2017-09-14T19:21:45.008532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1 # [1xD]\n",
    "b = 2*np.random.random()-1       # [1x1]\n",
    "\n",
    "for step in range(10001):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        # insira seu código aqui\n",
    "    \n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Numpy com Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:22:08.568244Z",
     "start_time": "2017-09-14T19:22:08.561239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))\n",
    "\n",
    "print(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x,y)\n",
    "\n",
    "print('w: ', reg.coef_)\n",
    "print('b: ', reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:22:33.763665Z",
     "start_time": "2017-09-14T19:22:33.556518Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1 # [1xD]\n",
    "b = 2*np.random.random()-1       # [1x1]\n",
    "\n",
    "learning_rate = 1.0 # qual o melhor valor de learning_rate agora?\n",
    "\n",
    "for step in range(1001):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        x_i = x_i.reshape(1, D)\n",
    "        y_pred = np.dot(x_i, w.T) + b\n",
    "        error = y_i - y_pred\n",
    "        w = w + learning_rate*np.dot(error.T, x_i)\n",
    "        b = b + learning_rate*error\n",
    "        sum_error += error\n",
    "    \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Exercício de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:07.079178Z",
     "start_time": "2017-09-15T10:56:06.991114Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/notas.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:12.063093Z",
     "start_time": "2017-09-15T10:56:08.355060Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sb.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:14.202826Z",
     "start_time": "2017-09-15T10:56:14.189835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = df[['prova1', 'prova2', 'prova3']].values\n",
    "y = df['final'].values.reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T10:56:15.149753Z",
     "start_time": "2017-09-15T10:56:15.143751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:24:23.821886Z",
     "start_time": "2017-09-14T19:24:23.678784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x,y)\n",
    "\n",
    "print('w: ', reg.coef_)\n",
    "print('b: ', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:24:36.348182Z",
     "start_time": "2017-09-14T19:24:33.850407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for d in range(D)] # [1xD]\n",
    "b = 2*random() - 1 # [1x1]\n",
    "\n",
    "learning_rate = 1.0 # <- tente encontrar a learning_rate!\n",
    "\n",
    "for step in range(2001):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        y_pred = sum([x_i[d]*w[d] for d in range(D)]) + b\n",
    "        error = y_i[0] - y_pred\n",
    "        w = [w[d] + learning_rate*error*x_i[d] for d in range(D)]\n",
    "        b = b + learning_rate*error\n",
    "        sum_error += error\n",
    "        \n",
    "    if step%200 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T19:24:55.296538Z",
     "start_time": "2017-09-14T19:24:54.907259Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1.0 # <- tente encontrar a learning_rate!\n",
    "\n",
    "for step in range(2001):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        x_i = x_i.reshape(1, D)\n",
    "        y_pred = np.dot(x_i, w.T) + b \n",
    "        error = y_i - y_pred\n",
    "        w = w + learning_rate*np.dot(error.T, x_i)\n",
    "        b = b + learning_rate*error\n",
    "        sum_error += error\n",
    "    \n",
    "    if step%200 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Artigo original do Perceptron](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
